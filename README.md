# 星影 (Hoshikage) - 高速ローカル推論サーバー

## 概要

**星影（ほしかげ）** は、GGUFフォーマットの大規模言語モデルをローカル環境で高速かつ効率的に実行し、OpenAI互換のAPIを提供するRustアプリケーションです。プライバシーを重視し、外部へのデータ送信を最小限に抑えつつ、高品質な対話型AI体験を提供します。

「**静かなる知性**」という設計思想のもと、必要な時にのみリソースを活用し、非アクティブ時には自動的にメモリを解放します。

---

## ✨ 特徴

### 🚀 高速推論
- **llama.cpp (動的リンク)**: システムCUDAライブラリを利用して高速推論
- **GPU加速**: CUDA対応（RTX 1650以降）
- **Flash Attention + KV Cache**: 推論速度を最大化
- **単一バイナリ**: 311KBのコンパクトなサイズ

### 🔌 OpenAI互換API
- **完全互換**: 既存のOpenAIクライアントライブラリがそのまま使用可能
- **ストリーミング対応**: リアルタイムで応答を逐次送信
- **複数モデル対応**: 複数のGGUFモデルを登録・切り替え可能

### 💡 リソース効率化（静かなる知性）
- **自動モデルアンロード**: 非アクティブ時に自動でメモリ解放
- **セマフォ制御**: 同時リクエスト数を1に制限してVRAM枯渇を防止

---

## 📋 必要要件

### ハードウェア

| 項目 | 最小要件 | 推奨要件 |
|------|---------|---------|
| CPU | 8コア以上 | 16コア以上（Ryzen 7900相当） |
| メモリ | 16GB以上 | 32GB以上 |
| GPU | VRAM 8GB以上 | VRAM 12GB以上 |
| ストレージ | SSD 50GB以上 | NVMe SSD 100GB以上 |

### ソフトウェア

- **OS**: Linux（Ubuntu 20.04以降推奨）
- **CUDAドライバ**: 470+ (GTX 1650以降)
- **Rust**: 1.70以上

---

## 🚀 インストール

### 依存関係のインストール

```bash
# Cargo経由でローカルインストール
cargo install --path .
```

これにより、星影バイナリと必要なライブラリがシステムにインストールされます。

---

## 🚀 実行

### 環境変数の設定

```bash
# システムCUDAライブラリのパスを設定
export LD_LIBRARY_PATH=/usr/local/cuda/targets/x86_64-linux/lib:$LD_LIBRARY_PATH
```

---

## 🏗️ アーキテクチャ

```
┌─────────────────────────────────┐
│         Rustバイナリ (311KB)          │
│  ┌────────────────────────────┐ │
│  │ Axum (OpenAI互換API)         │ │
│  ├────────────────────────────┤ │
│  │ llama.cpp (動的リンク)         │ │
│  └────────────────────────────┘ │
└─────────────────────────────────┘
                 │
                 │ システムCUDAライブラリ
                 ▼
┌─────────────────────────────────┐
│   CUDA Driver (動的リンク)            │
│   - libcuda.so                    │
│   - libcublas.so                  │
│   - libcudart.so                  │
└─────────────────────────────────┘
```

**動的リンクの仕組み:**
- llama.cppはシステムのCUDAライブラリを動的リンクして使用します
- 環境変数 `LD_LIBRARY_PATH` を設定して検索パスを指定

---

## 📊 パフォーマンス

| 指標 | 値 |
|-------|-----|
| バイナリサイズ | 311KB |
| 起動時間 | <1秒 |
| 初回モデルロード | 5-10秒 |
| モデルスイッチ | <1秒 |
| 推論速度 (RTX 4070 SUPER) | 30-50 tokens/s |

---

## 📝 ドキュメント

| ドキュメント | 説明 |
|-------------|------|
| [requirements.md](docs/requirements.md) | システム寄りの要件定義書 |
| [api-spec.md](docs/api-spec.md) | API仕様書 |
| [system-design.md](docs/system-design.md) | システム設計書 |
| [user-manual.md](docs/user-manual.md) | ユーザーマニュアル |
| [LIBRARY_FILES.md](docs/LIBRARY_FILES.md) | ライブラリファイルの配置について |
| [LIBRARY_PLACEMENT.md](docs/LIBRARY_PLACEMENT.md) | ライブラリ配置オプション |

---

## 🙏 謝辞

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - 高速推論エンジン
- [Axum](https://github.com/tokio-rs/axum) - 高速Webフレームワーク
- [Rust](https://www.rust-lang.org/) - システムプログラミング言語

---

**星影 - 暗闇の中で光を放つように、AI技術の可能性を照らす**
