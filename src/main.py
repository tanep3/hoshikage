import os
import asyncio
import time
from datetime import datetime
import logging
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from dotenv import load_dotenv
from llama_cpp import Llama
import mount as mt
from uuid import uuid4
from models.schema import ChatCompletionRequest, ChatSessionManager
import json
import gc
import re
import chromadb
from chroma_embedding_function import ChromaEmbeddingFunction
from select_sentence_representatives import select_sentence_representatives, split_and_clean_sentences
from fastapi.exceptions import RequestValidationError

VERSION = "0.1.0"

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ï¼‰
import pathlib
project_root = pathlib.Path(__file__).parent.parent
load_dotenv(dotenv_path=project_root / ".env")

def get_env_path(key, default_rel_path):
    val = os.getenv(key)
    if val:
        if val.startswith("./"):
            return str(project_root / val[2:])
        return val
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ã® models ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    return str(project_root / default_rel_path)

RAMDISK_PATH = os.getenv("RAMDISK_PATH", "/mnt/temp/hoshikage")
IDLE_TIMEOUT = int(os.getenv("IDLE_TIMEOUT_SECONDS", "300"))
GREAT_TIMEOUT = int(os.getenv("GREAT_TIMEOUT", "60")) * 60

# ãƒ«ãƒ¼ãƒˆã® data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹
MODEL_MAP_FILE = get_env_path("MODEL_MAP_FILE", "data/model_map.json")
TAG_CACHE_FILE = get_env_path("TAG_CACHE_FILE", "data/tags_cache.json")

# JSONã®å­˜åœ¨ç¢ºèªï¼ˆãªã‘ã‚Œã°ä½œæˆï¼‰
for fpath in [MODEL_MAP_FILE, TAG_CACHE_FILE]:
    p = pathlib.Path(fpath)
    if not p.exists():
        p.parent.mkdir(parents=True, exist_ok=True)
        with open(p, "w") as f:
            if "tags" in p.name:
                json.dump({"data": []} if "cache" in p.name else {"models": []}, f)
            else:
                json.dump({}, f)

# ChromaDBã®åˆæœŸåŒ–
CHROMA_PATH = get_env_path("CHROMA_PATH", "data/hoshikage_chroma_db")
SENTENCE_BERT_MODEL = os.getenv("SENTENCE_BERT_MODEL", "cl-nagoya/ruri-small-v2")
CHROMA_CLIENT = chromadb.PersistentClient(path=CHROMA_PATH)
EMBEDDING_FUNCTION = ChromaEmbeddingFunction(model_name=SENTENCE_BERT_MODEL)
CHROMA_SHORT_MEMORY_COLLECTION = CHROMA_CLIENT.get_or_create_collection(
    name="short_memory_db",
    embedding_function=EMBEDDING_FUNCTION
)

llm = None
llm_lock = asyncio.Lock()
concurrency_semaphore = asyncio.Semaphore(1) # åŒæ™‚å®Ÿè¡Œæ•°ã‚’1ã«åˆ¶é™
last_access_time = time.time()
# chat_session_manager = ChatSessionManager()
current_model = ""
IS_SEMAPHORE=False

async def initialize_model(model_alias):
    global llm, current_model
    if model_alias == current_model:
        if llm is not None:
            return
    current_model = model_alias
    if llm is not None:
        llm.close()
        llm = None
        gc.collect()
    ram_model_path = mt.get_model(current_model)
    llm = Llama(
        model_path=ram_model_path, 
        # n_ctx=20960,
        # n_ctx=12288,
        # n_ctx=10240,     # æ–‡è„ˆé•·ï¼šé•·ã‚ã§ã‚‚OKï¼ˆ4096ãŒæ¨å¥¨æœ€å¤§ï¼‰
        # n_ctx=9126,
        # n_ctx=8192,
        # n_ctx=5120,
        n_ctx=4096,
        n_threads=20,    # Ryzen 7900ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã«å¿œã˜ã¦ï¼ˆä¸Šé™ã¯è‡ªå‹•ã§ã‚‚è‰¯ã„ï¼‰
        n_gpu_layers=-1, # -1ã¯GPUã‚’Maxã¾ã§ä½¿ã†
        # n_gpu_layers=49, # -1ã¯GPUã‚’Maxã¾ã§ä½¿ã†
        # n_batch=1024,         # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå¤§ãã„ã¨é«˜é€Ÿãƒ»ãŸã ã—VRAMã«æ³¨æ„ï¼‰
        n_batch=512,
        use_mmap=True,   # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ RAM ã‚„ VRAM ã«å…¨ã¦èª­ã¿è¾¼ã‚€ä»£ã‚ã‚Šã«ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç›´æ¥ãƒ¡ãƒ¢ãƒªã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦åˆ©ç”¨ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚
        # type_k=7,     # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯f16
        # offload_kqv=False,   # Attention è¨ˆç®—ã®ä¸€éƒ¨ (K, Q, V ã®å°„å½±) ã‚’ CPU ã«æ‹…å½“ã•ã›ã‚‹
        verbose=False    # ğŸ‘ˆ å‡ºåŠ›ã‚’æŠ‘åˆ¶
    )

async def check_idle_timeout():
    global llm, last_access_time
    if time.time() - last_access_time > IDLE_TIMEOUT:
        async with llm_lock:
            if llm:
                logger.info("â³ éã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ™‚é–“è¶…éã®ãŸã‚ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
                llm.close()  # ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
                llm = None  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚¿ãŒè§£æ”¾ã—ã‚„ã™ãã™ã‚‹
                gc.collect()
    if time.time() - last_access_time > GREAT_TIMEOUT:
        mt.unmount_ramdisk(RAMDISK_PATH)

@app.get("/v1/status")
async def status():
    return {"status": "ok"}

@app.get("/v1/models")
async def get_model_tags():
    try:
        if not os.path.exists(TAG_CACHE_FILE):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {TAG_CACHE_FILE}")
        with open(TAG_CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"/api/tags ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "code": "tags_fetch_failed",
                    "message": str(e),
                    "type": "internal_server_error"
                }
            }
        )

@app.get("/v1/api/version")
async def get_version():
    return {"version": VERSION}

# âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿å®šç¾©
def stream_generator(current_model, prompt, session_id):
    global IS_SEMAPHORE
    try:
        partial_text = ""
        for chunk in llm(
            prompt,
            max_tokens=2096,
            stop=["<|eot|>", "user:", "<|user|>", "</|assistant|>", "<|endoftext|>", "Q:"],
            # stop=["<|eot|>", "user:", "User:", "Assistant:", "assistant:"],
            # "<|user|>","</|assistant|>"
            stream=True
        ):
            delta = chunk.get("choices", [{}])[0].get("text", "")
            if not delta:
                continue
            partial_text += delta
            # OpenAIäº’æ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            payload = {
                "id": f"chatcmpl-{uuid4().hex}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": current_model,
                "choices": [{
                    "delta": {"content": delta},
                    "index": 0,
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(payload)}\n\n"
        # âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®çµ‚äº†é€šçŸ¥
        yield "data: [DONE]\n\n"
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¦ç´„ã—ã¦Chromaã«ä¿å­˜
        # history_message = message_compress("assistant", partial_text)
        # is_compressed = history_message != partial_text
        # save_chroma("assistant", history_message, is_compressed)
        IS_SEMAPHORE = False

    except Exception as e:
        logger.error(f"Streaming error: {e}")
        IS_SEMAPHORE = False
        yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"
    finally:
        IS_SEMAPHORE = False

# éã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¤ãƒ³ã‚°ç”¨
async def non_streaming_generator(current_model, prompt, session_id):
    output = llm(
        prompt, 
        max_tokens=1024, 
        stop=["<|eot|>", "<|endoftext|>", "user:", "Q:"],
    )
    assistant_message = output["choices"][0]["text"]

    usage = output.get("usage", {})
    response = {
        "id": f"chatcmpl-{uuid4().hex}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": current_model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": assistant_message
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0)
        }
    }
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¦ç´„ã—ã¦Chromaã«ä¿å­˜
    # history_message = message_compress("assistant", assistant_message)
    # is_compressed = history_message != assistant_message
    # save_chroma("assistant", history_message, is_compressed)

    return response

def save_chroma(role, message, is_compressed):
    if not message:
        return
    try:
        if read_chroma(message):
            # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ä¿å­˜ã—ãªã„
            return
        CHROMA_SHORT_MEMORY_COLLECTION.add(
            documents=[message],
            metadatas=[{
                "role": role,
                "create_date": datetime.now().strftime("%Y%m%d"),
                "create_time": datetime.now().strftime("%H:%M:%S"),
                "compressed": is_compressed
            }],
            ids=[uuid4().hex]
        )
    except Exception as e:
        logger.error(f"ChromaDBã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def read_chroma(message):
    if not message:
        return None
    try:
        query_results = CHROMA_SHORT_MEMORY_COLLECTION.query(
            query_texts=[message],
            n_results=1
            # include=["distances", "documents"],
        )
        print(f"ğŸ” Chromaæ¤œç´¢ã‚¯ã‚¨ãƒª: {message}")
        distances = query_results.get("distances", [[]])
        if distances and distances[0]:
            if distances[0][0]:
                if distances[0][0] >= 0.15:
                    print("âŒï¸ ãƒ’ãƒƒãƒˆã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
                    print(f"âœˆï¸ è·é›¢: {distances[0][0]}")
                    print(query_results["documents"][0][0])
                    return None
        print(f"ğŸ” Chromaæ¤œç´¢çµæœ: {query_results}")
        return query_results["documents"][0][0] if query_results["documents"] else None
    except Exception as e:
        logger.error(f"ChromaDBã®ã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼: {e}")
        return None

def message_compress(role: str, message: str) -> str:
    if "```python" in message or role == "system":
        return message  # ã‚³ãƒ¼ãƒ‰ã‚„ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã¯ãã®ã¾ã¾ä¿æŒ
    if len(message) <= 150:
        return message  # 150æ–‡å­—ä»¥ä¸‹ã¯ç„¡åœ§ç¸®
    compress_prompt = f"system: æ¬¡ã®å†…å®¹ã‚’150æ–‡å­—ç¨‹åº¦ã«è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚æ–‡ç« ã®ä¸»ãŸã‚‹æ„å›³ã‚’å¤±ã‚ãªã„ã‚ˆã†ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚èªèª¿ã‚„è¨€ã„å›ã—ãªã©ä¼šè©±è¡¨ç¾çš„ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ãŒæœ‰ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ç¶­æŒã™ã‚‹ã‚ˆã†å¿ƒãŒã‘ã¦ä¸‹ã•ã„ã€‚å›ç­”ã¯è¦ç´„ã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®æƒ…å ±ã¯ä»˜ä¸ã—ãªã„ã“ã¨ã€‚Markdownè¡¨è¨˜ã¯å¹³æ–‡ã«ç›´ã—ã¦ä¸‹ã•ã„ã€‚åˆ¶å¾¡ã‚³ãƒ¼ãƒ‰ã¯å‰Šé™¤ã—ã¦ã€æ–‡å­—æ•°å‰Šæ¸›ã«å‹™ã‚ã‚‹ã“ã¨ã€‚\nuser: {message}\nassistant: "
    output = llm(
        compress_prompt, 
        max_tokens=256, 
        stop=["<|eot|>", "user:"],
    )
    # æ–‡æœ«ã®ã‚´ãƒŸã‚’é™¤å»ã—ã¦ã‹ã‚‰è¿”ã™
    result = re.sub(r"(```+|[\[\(\{]*$)", "", output["choices"][0]["text"]).strip()
    return result

@app.exception_handler(RequestValidationError)
async def handler(request:Request, exc:RequestValidationError):
    print(exc)
    return JSONResponse(content={}, status_code=422)

@app.post("/v1/chat/completions")
async def create_completion(completion_data: ChatCompletionRequest):
# async def create_completion(completion_data):
    global last_access_time, IS_SEMAPHORE  #, chat_session_manager
    sleep_count = 0
    while IS_SEMAPHORE:
        if sleep_count > 1800: # 180ç§’å¾…ã£ã¦ã‚‚è§£æ”¾ã•ã‚Œãªã„å ´åˆã¯å¼·åˆ¶çµ‚äº†
            IS_SEMAPHORE = False
            LLM.close()
            LLM = None
            gc.collect()
            break
        await asyncio.sleep(0.1)
        sleep_count += 1
    IS_SEMAPHORE = True
    async with concurrency_semaphore:
        model_alias = completion_data.model
        await initialize_model(model_alias)
        last_access_time = time.time()
        # print("#########################")
        # print(f"model_alias: {model_alias}")
        # print(f"stream: {completion_data.stream}")
        # print(f"messages: {completion_data}")

        messages = completion_data.messages
        session_id = messages[0].session_id if hasattr(messages[0], "session_id") else "default_session"

        prompt = ""
        system_prompt = ""
        user_prompt = ""
        all_histories = ""
        prompt_raw = ""
        raw_talks = 3
        talks_count = 0
        # æœ€å¾Œã®raw_talkså¾€å¾©ã¯ã€Œè¦ç´„å¯¾è±¡ã‹ã‚‰é™¤å¤–ã€ã—ã¦åŸæ–‡ã®ã¾ã¾é€£çµã™ã‚‹ã“ã¨ã§æ–‡è„ˆã‚’ä¿æŒ
        for msg in messages[::-1]:
            if msg.role == "system":
                system_prompt += msg.content + "\n"
                continue
            if user_prompt == "":
                if msg.role == "user":
                    user_prompt = msg.content
                    continue
            if talks_count < raw_talks:
                # ç›´è¿‘ã®raw_talkså¾€å¾©ã¯ãã®ã¾ã¾é€£çµ
                prompt_raw += f"{msg.role}: {msg.content}\n"
                if msg.role == "user":
                    talks_count += 1
                continue
            all_histories += msg.content + "\n"
        if all_histories:
            prompt = select_sentence_representatives(split_and_clean_sentences(all_histories), EMBEDDING_FUNCTION)
        if prompt:
            prompt = f"## **(å‚è€ƒæƒ…å ±)ä¼šè©±ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ** \n{prompt}\n" 
        if prompt_raw:
            prompt += f"## **(å‚è€ƒæƒ…å ±)ç›´è¿‘ã®ä¼šè©±å±¥æ­´** \n{prompt_raw}\n"
        if prompt:
            prompt += "\n## **æœ¬é¡Œï¼ˆä»¥ä¸‹ã«ä¼šè©±ã‚’ç¶šã‘ã¦ä¸‹ã•ã„ã€‚ï¼‰** \n"
        if user_prompt:
            prompt += f"user: {user_prompt}\n"
        if system_prompt:
            prompt = f"system: {system_prompt}\n" + prompt
        prompt += "\nassistant: "
        print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
        print(prompt)

        """
        prompt = ""
        system_prompt = ""
        user_prompt = ""
        for msg in messages[::-1]:
            if msg.role == "system":
                system_prompt += msg.content
                continue
            if user_prompt == "":
                if msg.role == "user":
                    user_prompt = msg.content
                    continue
            history_message = read_chroma(msg.content)
            if not history_message:
                # Chromaã«ç„¡ã„å ´åˆã¯ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãã®ã¾ã¾ä½¿ç”¨
                history_message = msg.content
            if msg.content != history_message:
                print("#########################")
                print(f"role: {msg.role}")
                print(f"original: {msg.content}")
                print("---")
                print(f"compress: {history_message}")
            prompt = f"{msg.role}: {history_message}\n" + prompt
            if len(prompt) > 5120:
                break
        if prompt:
            prompt = (
                "## **(å‚è€ƒæƒ…å ±)ä¼šè©±ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ** \n" 
                + prompt + 
                "\n## **æœ¬é¡Œï¼ˆä»¥ä¸‹ã«ä¼šè©±ã‚’ç¶šã‘ã¦ä¸‹ã•ã„ã€‚ï¼‰** \n"
            )
        if user_prompt:
            prompt += f"user: {user_prompt}\n"
            # ğŸ§  æœ€æ–°ã®userç™ºè¨€ã ã‘ã‚’ã€RAGå±¥æ­´ã¨ã—ã¦ä¿æŒã—ã¦ãŠã
            compressed_message = message_compress("user", user_prompt)
            is_compressed = compressed_message != user_prompt
            save_chroma("user", compressed_message, is_compressed)
        if system_prompt:
            prompt = f"system: {system_prompt}\n" + prompt
        prompt += "assistant: "
        print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
        print(prompt)
        """

        if completion_data.stream:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆ
            return StreamingResponse(stream_generator(current_model, prompt, session_id), media_type="text/event-stream")
        else:
            response = await non_streaming_generator(current_model, prompt, session_id)
            print(response)
            IS_SEMAPHORE = False
            return response
        
async def background_cleanup():
    while True:
        await asyncio.sleep(30) #30ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
        await check_idle_timeout()

@app.on_event("startup")
async def startup_event():
    asyncio.create_task(background_cleanup())

