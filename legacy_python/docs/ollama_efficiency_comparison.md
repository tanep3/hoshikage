# Ollamaã¨Hoshikageã®LLMåŠ¹ç‡åŒ–æ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

ä½œæˆæ—¥: 2026-01-16

## æ¦‚è¦

æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯ã€Ollamaãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆhttps://github.com/ollama/ollamaï¼‰ã¨Hoshikageãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆ/home/tane/dev/AI/hoshikageï¼‰ã®LLMæ“ä½œã«ãŠã‘ã‚‹åŠ¹ç‡åŒ–æ‰‹æ³•ã‚’æ¯”è¼ƒåˆ†æã—ãŸã‚‚ã®ã§ã™ã€‚

---

## Hoshikageã®ç¾åœ¨ã®å®Ÿè£…

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **è¨€èª**: Python
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: FastAPI
- **LLMã‚¨ãƒ³ã‚¸ãƒ³**: llama-cpp-python
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 0.1.0

### ç¾åœ¨ã®åŠ¹ç‡åŒ–æ‰‹æ³•

#### 1. RAMãƒ‡ã‚£ã‚¹ã‚¯ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é«˜é€ŸåŒ–
- ãƒ¢ãƒ‡ãƒ«ã‚’RAMãƒ‡ã‚£ã‚¹ã‚¯ï¼ˆ`/mnt/temp/hoshikage`ï¼‰ã«ãƒã‚¦ãƒ³ãƒˆ
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®ä»£ã‚ã‚Šã«ãƒ¡ãƒ¢ãƒªã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ã§é«˜é€ŸåŒ–
- å®Ÿè£…å ´æ‰€: `src/mount.py` åŠã³ `main.py:122-130`

```python
self.llm = Llama(
    model_path=ram_model_path,
    n_ctx=N_CTX,
    n_threads=N_THREADS,
    n_gpu_layers=N_GPU_LAYERS,
    n_batch=N_BATCH,
    use_mmap=True,  # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
    verbose=False
)
```

#### 2. ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«ã‚ˆã‚‹è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
- éã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ™‚é–“ãŒ300ç§’ï¼ˆè¨­å®šå¯èƒ½ï¼‰ã‚’è¶…ãˆã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
- RAMãƒ‡ã‚£ã‚¹ã‚¯ã‚‚60åˆ†å¾Œã«ã‚¢ãƒ³ãƒã‚¦ãƒ³ãƒˆ
- å®Ÿè£…å ´æ‰€: `main.py:145-159`

```python
async def check_idle_timeout(self) -> None:
    if time.time() - self.last_access_time > IDLE_TIMEOUT:
        self.llm.close()
        self.llm = None
        gc.collect()
```

#### 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†ã¨è¦ç´„
- ç›´è¿‘ã®ä¼šè©±å±¥æ­´ã¯åŸæ–‡ã‚’ç¶­æŒ
- å¤ã„å±¥æ­´ã¯æ–‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§è¦ç´„ï¼ˆ`select_sentence_representatives`ï¼‰
- ChromaDBã«ã‚ˆã‚‹é¡ä¼¼æ–‡æ¤œç´¢ã§é‡è¤‡ã‚’æ’é™¤
- å®Ÿè£…å ´æ‰€: `main.py:401-434`

```python
if all_histories:
    prompt = select_sentence_representatives(
        split_and_clean_sentences(all_histories),
        EMBEDDING_FUNCTION,
        cluster_divisor=CLUSTER_DIVISOR,
        min_clusters=MIN_CLUSTERS,
        max_clusters=MAX_CLUSTERS
    )
```

#### 4. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åœ§ç¸®
- 150æ–‡å­—ã‚’è¶…ãˆã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è‡ªå‹•çš„ã«LLMã§è¦ç´„
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯åœ§ç¸®å¯¾è±¡å¤–
- å®Ÿè£…å ´æ‰€: `main.py:349-362`

#### 5. å˜ä¸€ãƒ¢ãƒ‡ãƒ«ç®¡ç†
- ä¸€åº¦ã«1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
- ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆæ™‚ã«å‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«ã‚¯ãƒ­ãƒ¼ã‚º
- æ’ä»–åˆ¶å¾¡ã¯ `asyncio.Semaphore(1)` ã§å®Ÿè£…

### åˆ¶ç´„äº‹é …

1. **åŒæ™‚å®Ÿè¡Œåˆ¶é™**: 1ãƒ¢ãƒ‡ãƒ«ã®ã¿ã€ä¸¦åˆ—å‡¦ç†ãªã—
2. **VRAMç®¡ç†**: æ‰‹å‹•è¨­å®šï¼ˆ`N_GPU_LAYERS`ï¼‰ã«ä¾å­˜
3. **ãƒ¡ãƒ¢ãƒªå›å¾©å¾…æ©Ÿ**: GPUãƒ¡ãƒ¢ãƒªè§£æ”¾ã®å¾…æ©Ÿãªã—
4. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–**: KV Cacheã®é‡å­åŒ–ãªã©æœªå®Ÿè£…

---

## Ollamaã®åŠ¹ç‡åŒ–æ‰‹æ³•

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **è¨€èª**: Go
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: æ¨™æº–Go HTTPã‚µãƒ¼ãƒãƒ¼ + CGO
- **LLMã‚¨ãƒ³ã‚¸ãƒ³**: llama-cppï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒ“ãƒ«ãƒ‰ï¼‰+ ç‹¬è‡ªã‚¨ãƒ³ã‚¸ãƒ³

### ä¸»è¦ãªåŠ¹ç‡åŒ–æ‰‹æ³•

#### 1. é«˜åº¦ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼ˆSchedulerï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/sched.go`

Ollamaã¯è¤‡é›‘ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè£…ã—ã€è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªç®¡ç†ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

##### å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã«ã‚ˆã‚‹ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†
```go
type runnerRef struct {
    refMu    sync.Mutex
    refCount uint  // prevent unloading if > 0
    // ... other fields
}
```

- `refCount`ã§ç¾åœ¨ã®ä½¿ç”¨ä¸­ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’è¿½è·¡
- ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã¯ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰ä¿è­·
- ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†æ™‚ã«è‡ªå‹•çš„ã«æ¸›å°‘

##### æ™ºèƒ½çš„ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰æˆ¦ç•¥
```go
func (s *Scheduler) findRunnerToUnload() *runnerRef {
    // Sort by session duration and name
    sort.Sort(ByDurationAndName(runnerList))
    // Try to find idle runner first
    for _, runner := range runnerList {
        if runner.refCount == 0 {
            return runner
        }
    }
    // No idle runners, pick shortest duration
    return runnerList[0]
}
```

- ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ã®çŸ­ã„ãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆã—ã¦ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
- ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
- `defaultModelsPerGPU = 3`ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§GPUã‚ãŸã‚Š3ãƒ¢ãƒ‡ãƒ«ã¾ã§è¨±å®¹ï¼‰

##### è¤‡æ•°GPUå¯¾å¿œ
- è¤‡æ•°ã®GPUã‚’ä½¿ç”¨ã—ãŸå ´åˆã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æœ€é©ã«åˆ†é…
- GPUãƒ©ã‚¤ãƒ–ãƒ©ãƒªã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆCUDA, ROCm, Vulkan, Metalï¼‰
- å„GPUã®ç©ºããƒ¡ãƒ¢ãƒªã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç›£è¦–

#### 2. VRAMå›å¾©å¾…æ©Ÿæ©Ÿæ§‹

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/sched.go:waitForVRAMRecovery`

```go
func (s *Scheduler) waitForVRAMRecovery(...) chan any {
    // Establish baseline before unload
    gpusBefore := s.getGpuFn(context.Background(), runners)

    go func() {
        ctx, cancel := context.WithTimeout(context.Background(), s.waitForRecovery)
        defer cancel()
        ticker := time.NewTicker(250 * time.Millisecond)

        for {
            select {
            case <-ticker.C:
                // Query GPUs, look for free to go back up
                gpusNow := s.getGpuFn(ctx, runners)
                freeMemoryNow := calculateFree(gpusNow)

                // If we're within ~75% of estimated memory usage recovered, bail out
                if float32(freeMemoryNow-freeMemoryBefore) > float32(runner.vramSize)*0.75 {
                    finished <- struct{}{}
                    return
                }
            case <-ctx.Done():
                finished <- struct{}{}
                return
            }
        }
    }()
    return finished
}
```

**ç‰¹å¾´**:
- GPUãƒ¡ãƒ¢ãƒªã®è§£æ”¾ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°ã§ç›£è¦–ï¼ˆ250msé–“éš”ï¼‰
- æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®75%å›å¾©ã—ãŸæ™‚ç‚¹ã§å®Œäº†
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾Œã¯æ¨å®šå€¤ã«ä¿¡é ¼ã—ã¦ç¶šè¡Œ
- CPU, Metal, iGPUã¯å¾…æ©Ÿãªã—ï¼ˆå³æ™‚å®Œäº†ï¼‰

**åŠ¹æœ**:
- æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã®VRAMä¸è¶³ã‚’é˜²ã
- ãƒ¡ãƒ¢ãƒªå ±å‘Šã®é…å»¶ï¼ˆCUDAãªã©ï¼‰ã«å¯¾å¿œ

#### 3. å‹•çš„ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–

**ãƒ•ã‚¡ã‚¤ãƒ«**: `llm/server.go:load` and `llm/server.go:createLayout`

Ollamaã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã€ä»¥ä¸‹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚

##### ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ±ºå®š
```go
for {
    var runnerToExpire *runnerRef

    // Get current loaded runners
    runner := s.loaded[pending.model.ModelPath]

    if runner != nil {
        if runner.needsReload(ctx, pending) {
            runnerToExpire = runner
        } else {
            // Use existing runner
            pending.useLoadedRunner(runner, s.finishedReqCh)
            break
        }
    } else if maxRunners > 0 && loadedCount >= int(maxRunners) {
        runnerToExpire = s.findRunnerToUnload()
    } else {
        // Try to fit model
        gpus := s.getGpuFn(ctx, runnersSnapshot)
        systemInfo := s.getSystemInfoFn()

        needEvict := s.loadFn(pending, ggml, systemInfo, gpus, true)
        if !needEvict {
            break  // Model fits with existing models
        }
        runnerToExpire = s.findRunnerToUnload()
    }

    if runnerToExpire != nil {
        // Expire and wait for unload
        runnerToExpire.sessionDuration = 0
        s.expiredCh <- runnerToExpire
        <-s.unloadedCh
        continue
    }
}
```

##### ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºè¨ˆç®—ã¨æœ€é©åŒ–
```go
kv, graphPartialOffload, graphFullOffload := s.ggml.GraphSize(
    uint64(s.options.NumCtx),
    uint64(s.loadRequest.BatchSize),
    s.loadRequest.Parallel,
    s.loadRequest.KvCacheType,
    s.loadRequest.FlashAttention,
)

for _, gl := range ml.ByLibrary(gpus) {
    gpuLayers = assignLayers(layers, gl, requireFull, s.options.NumGPU, lastUsedGPU)
    if gpuLayers.Sum() > currentMax {
        currentMax = gpuLayers.Sum()
    }
}
```

- å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚µã‚¤ã‚ºã¨KV Cacheã‚µã‚¤ã‚ºã‚’è¨ˆç®—
- `assignLayers`ã§è²ªæ¬²æ³•ï¼ˆGreedy Fitï¼‰ã«ã‚ˆã‚Šå‰²ã‚Šå½“ã¦
- GPUãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¨®é¡ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦æœ€é©åŒ–

##### ãƒãƒ«ãƒGPUã¸ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼å‰²ã‚Šå½“ã¦
```go
func assignLayers(layers []uint64, gpus []ml.DeviceInfo, requireFull bool, requestedLayers int, lastUsedGPU int) (gpuLayers ml.GPULayersList) {
    // Pack layers into as few GPUs as possible
    for i := lastUsedGPU; i < len(gpus); i++ {
        gpuLayers = findBestFit(layers, gpus[:i+1], requestedLayers, forceRequest)
        if gpuLayers.Sum() == len(layers) || gpuLayers.Sum() == requestedLayers {
            break
        }
    }
    return gpuLayers
}
```

- **è²ªæ¬²ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: ç©ºãå®¹é‡ã®å¤§ãã„GPUã‹ã‚‰é †ã«å‰²ã‚Šå½“ã¦
- **éƒ¨åˆ†ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰**: VRAMä¸è¶³æ™‚ã¯ä¸€éƒ¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’CPUã«
- è¤‡æ•°GPUã‚’åŠ¹ç‡çš„ã«æ´»ç”¨

#### 4. Flash Attentionã¨KV Cacheé‡å­åŒ–

**ãƒ•ã‚¡ã‚¤ãƒ«**: `llm/server.go`

##### Flash Attention
```go
fa := envconfig.FlashAttention(f.FlashAttention())

if fa && !ml.FlashAttentionSupported(gpus) {
    slog.Warn("flash attention enabled but not supported by gpu")
    fa = false
}

if fa && !f.SupportsFlashAttention() {
    slog.Warn("flash attention enabled but not supported by model")
    fa = false
}

loadRequest.FlashAttention = flashAttention
```

- å…¨GPUãŒFlash Attentionã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã®ã¿æœ‰åŠ¹åŒ–
- ãƒ¢ãƒ‡ãƒ«ã¨GPUã®ä¸¡æ–¹ã®äº’æ›æ€§ã‚’ç¢ºèª

##### KV Cacheé‡å­åŒ–
```go
kvct := strings.ToLower(envconfig.KvCacheType())

if textProcessor == nil {
    if kvct != "" {
        if f.KVCacheTypeIsQuantized(kvct) {
            if flashAttention != ml.FlashAttentionEnabled {
                slog.Warn("OLLAMA_FLASH_ATTENTION must be enabled to use quantized OLLAMA_KV_CACHE_TYPE")
            } else if f.SupportsKVCacheType(kvct) {
                loadRequest.KvCacheType = kvct
            }
        }
    }
}
```

- Flash Attentionæœ‰åŠ¹æ™‚ã®ã¿KV Cacheã®é‡å­åŒ–ã‚’è¨±å®¹
- ç’°å¢ƒå¤‰æ•° `OLLAMA_KV_CACHE_TYPE` ã§è¨­å®šå¯èƒ½

#### 5. ä¸¦åˆ—å‡¦ç†ï¼ˆMulti-User Cacheï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `llm/server.go`

```go
numParallel := max(int(envconfig.NumParallel()), 1)

// Embedding models should always be loaded with parallel=1
if req.model.CheckCapabilities(model.CapabilityCompletion) != nil {
    numParallel = 1
}

loadRequest.Parallel = numParallel
```

- åŒä¸€ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦è¤‡æ•°ã®ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨±å®¹
- `OLLAMA_NUM_PARALLEL` ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡
- Embeddingãƒ¢ãƒ‡ãƒ«ã¯å¸¸ã« `parallel=1`ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç«¶åˆã‚’é˜²ããŸã‚ï¼‰

#### 6. ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢ã«ã‚ˆã‚‹å®‰å®šæ€§ç¢ºä¿

**ãƒ•ã‚¡ã‚¤ãƒ«**: `llm/server.go`

```go
cmd := exec.Command(exe, params...)

// Create subprocess with stdout/stderr pipes
stdout, err := cmd.StdoutPipe()
stderr, err := cmd.StderrPipe()

go func() {
    io.Copy(out, stdout)
}()
go func() {
    io.Copy(out, stderr)
}()

cmd.Start()
```

- llama.cppãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦èµ·å‹•
- HTTPï¼ˆlocalhost:portï¼‰ã§é€šä¿¡
- ãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã‚‚ã‚µãƒ¼ãƒãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã¯ç”Ÿå­˜

#### 7. KV Cacheãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆå°‚ç”¨å®Ÿè£…ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `kvcache/cache.go`, `kvcache/causal.go`, `kvcache/encoder.go`

Ollamaã¯KV Cacheã®ç‹¬è‡ªå®Ÿè£…ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

##### Causal Attentionã®æœ€é©åŒ–
- `kvcache/causal.go` ã§Causal Attentionã®ç‰¹æ®ŠåŒ–æœ€é©åŒ–
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªã‚­ãƒ¼ç”Ÿæˆ

##### ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- `kvcache/cache.go` ã§è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸¦åˆ—ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚µãƒãƒ¼ãƒˆ
- Lock-freeãªã‚¢ã‚¯ã‚»ã‚¹ã§ã‚³ãƒ³ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æœ€å°åŒ–

---

## æ¯”è¼ƒåˆ†æ

### åŠ¹ç‡åŒ–æ‰‹æ³•ã®æ¯”è¼ƒè¡¨

| æ‰‹æ³• | Hoshikage | Ollama | å·®åˆ† |
|------|------------|---------|------|
| **ãƒ¢ãƒ‡ãƒ«ç®¡ç†** | å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã€ã‚»ãƒãƒ•ã‚©ã‚¢åˆ¶å¾¡ | è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã€å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã€ã‚¹ãƒãƒ¼ãƒˆã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ | Ollamaã¯è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’åŒæ™‚ç®¡ç†å¯èƒ½ |
| **VRAMå›å¾©å¾…æ©Ÿ** | ãªã— | ãƒãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å¾…æ©Ÿæ©Ÿæ§‹ï¼ˆ75%å›å¾©ã§å®Œäº†ï¼‰ | Ollamaã¯GPUãƒ¡ãƒ¢ãƒªè§£æ”¾ã‚’å¾…ã£ã¦ã‹ã‚‰æ¬¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ |
| **ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ** | æ‰‹å‹•è¨­å®šï¼ˆN_GPU_LAYERSï¼‰ | ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–ãªå‹•çš„æœ€é©åŒ– | Ollamaã¯VRAMã«åˆã‚ã›ã¦è‡ªå‹•æœ€é© |
| **ãƒãƒ«ãƒGPUå¯¾å¿œ** | åŸºæœ¬çš„ã«å¯¾å¿œ | è¤‡æ•°GPUã¸ã®å‹•çš„ãƒ¬ã‚¤ãƒ¤ãƒ¼å‰²ã‚Šå½“ã¦ | Ollamaã¯è¤‡æ•°GPUã‚’åŠ¹ç‡çš„ã«æ´»ç”¨ |
| **Flash Attention** | llama-cppã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ | æ¡ä»¶ä»˜ãæœ‰åŠ¹åŒ– + KV Cacheé‡å­åŒ– | Ollamaã¯æ˜ç¤ºçš„ã«æœ€é©åŒ– |
| **ä¸¦åˆ—å‡¦ç†** | ãªã—ï¼ˆã‚»ãƒãƒ•ã‚©=1ï¼‰ | Multi-User Cacheã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç† | Ollamaã¯åŒä¸€ãƒ¢ãƒ‡ãƒ«ã§ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¯èƒ½ |
| **ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢** | åŒä¸€ãƒ—ãƒ­ã‚»ã‚¹å†… | ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ + HTTPé€šä¿¡ | Ollamaã¯ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã«å¼·é­ |
| **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–** | ChromaDBã«ã‚ˆã‚‹æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | KV Cacheé‡å­åŒ– + ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | Ollamaã¯KV Cacheãƒ¬ãƒ™ãƒ«ã§æœ€é©åŒ– |
| **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†** | æ–‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§è¦ç´„ | llama.cppã®æ¨™æº–æ©Ÿèƒ½ | Hoshikageã¯ç‹¬è‡ªã®è¦ç´„ãƒ­ã‚¸ãƒƒã‚¯ã‚’æŒã¤ |

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¯”è¼ƒ

| é …ç›® | Hoshikage | Ollama |
|------|------------|---------|
| **è¨€èª** | Python | Go |
| **ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«** | åŒä¸€ãƒ—ãƒ­ã‚»ã‚¹ | ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢ |
| **é€šä¿¡æ–¹å¼** | é–¢æ•°å‘¼ã³å‡ºã— | HTTPï¼ˆlocalhostï¼‰ |
| **LLMã‚¨ãƒ³ã‚¸ãƒ³** | llama-cpp-python | llama-cppï¼ˆCGOï¼‰+ ç‹¬è‡ªã‚¨ãƒ³ã‚¸ãƒ³ |
| **è¨­å®šæ–¹æ³•** | ç’°å¢ƒå¤‰æ•° + .env | ç’°å¢ƒå¤‰æ•° + ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ + Modelfile |
| **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ** | Pythonã§ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å®¹æ˜“ | ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒã‚¤ãƒŠãƒªã§å„OSã«æœ€é©åŒ– |

---

## Hoshikageã¸ã®é©ç”¨æ¨å¥¨äº‹é …

### å„ªå…ˆåº¦é«˜ï¼ˆå³æ™‚å®Ÿè£…å¯èƒ½ï¼‰

#### 1. VRAMå›å¾©å¾…æ©Ÿæ©Ÿæ§‹ã®å°å…¥

**å®Ÿè£…å ´æ‰€**: `src/main.py`

ç¾åœ¨ã®Hoshikageã¯ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸå¾Œã€GPUãƒ¡ãƒ¢ãƒªã®è§£æ”¾ã‚’å¾…ãŸãšã«æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚ã“ã‚Œã¯VRAMä¸è¶³ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

```python
import time
import subprocess

def wait_for_vram_recovery(gpus, vram_size, timeout=5):
    """
    GPUãƒ¡ãƒ¢ãƒªã®è§£æ”¾ã‚’å¾…æ©Ÿã™ã‚‹é–¢æ•°

    Args:
        gpus: GPUãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        vram_size: æ¨å®šè§£æ”¾ãƒ¡ãƒ¢ãƒªé‡ï¼ˆãƒã‚¤ãƒˆï¼‰
        timeout: æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
    """
    start = time.time()
    baseline_free = get_gpu_free_memory(gpus)

    while time.time() - start < timeout:
        current_free = get_gpu_free_memory(gpus)
        recovered = current_free - baseline_free

        # æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®75%ä»¥ä¸Šå›å¾©ã—ãŸã‚‰å®Œäº†
        if recovered > vram_size * 0.75:
            logger.info(f"âœ… VRAM recovered: {recovered / 1024**3:.2f}GB / {vram_size / 1024**3:.2f}GB")
            return True

        time.sleep(0.25)  # 250msé–“éš”ã§ãƒãƒ¼ãƒªãƒ³ã‚°

    logger.warning(f"âš ï¸  VRAM recovery timeout after {timeout}s")
    return False
```

**åŠ¹æœ**:
- å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’é€£ç¶šã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã®VRAMä¸è¶³ã‚’é˜²ã
- ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã®æˆåŠŸç‡å‘ä¸Š

#### 2. å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ç®¡ç†

**å®Ÿè£…å ´æ‰€**: `src/main.py` ã® `ModelManager` ã‚¯ãƒ©ã‚¹

ç¾åœ¨ã®Hoshikageã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Œäº†ã‚’å¾…ã£ã¦ã‹ã‚‰ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰åˆ¤å®šã‚’è¡Œã£ã¦ã„ã¾ã™ãŒã€å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã‚’è¿½è·¡ã—ã¦ã„ã¾ã›ã‚“ã€‚

```python
class ModelManager:
    def __init__(self):
        self.llm: Optional[Llama] = None
        self.llm_lock = asyncio.Lock()
        self.concurrency_semaphore = asyncio.Semaphore(1)
        self.last_access_time = time.time()
        self.current_model = ""
        self.current_model_config: Dict[str, Any] = {}
        self.is_processing = False
        self.ref_count = 0  # æ–°è¦è¿½åŠ 

    async def acquire(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã‚’å¢—åŠ """
        async with self.llm_lock:
            self.ref_count += 1
            self.last_access_time = time.time()

    async def release(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¸›å°‘ã—ã€å¿…è¦ãªã‚‰ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        async with self.llm_lock:
            self.ref_count -= 1

            if self.ref_count == 0 and time.time() - self.last_access_time > IDLE_TIMEOUT:
                # å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆãŒ0ã§ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®å ´åˆã®ã¿ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
                logger.info("ğŸ”„ Model idle timeout, unloading...")
                if self.llm:
                    self.llm.close()
                    self.llm = None
                gc.collect()
```

**åŠ¹æœ**:
- ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«ãŒèª¤ã£ã¦ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã®ã‚’é˜²ã
- è¤‡æ•°ã®ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®‰å…¨ã«å‡¦ç†å¯èƒ½ã«ãªã‚‹åŸºç›¤

#### 3. Flash Attentionã¨KV Cacheé‡å­åŒ–ã®æœ‰åŠ¹åŒ–

**å®Ÿè£…å ´æ‰€**: `src/main.py`

llama-cppã®è¨­å®šã§Flash Attentionã¨KV Cacheé‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã™ã€‚

```python
# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æ™‚
self.llm = Llama(
    model_path=ram_model_path,
    n_ctx=N_CTX,
    n_threads=N_THREADS,
    n_gpu_layers=N_GPU_LAYERS,
    n_batch=N_BATCH,
    use_mmap=True,
    f16_kv=True,  # KV Cacheã‚’f16ã§é‡å­åŒ–
    verbose=False
)

# Flash Attentionï¼ˆllama-cppã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã‚‹ï¼‰
# æ³¨: llama-cpp-pythonã§Flash Attentionã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã«ã¯
# æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå¿…è¦
```

**ç’°å¢ƒå¤‰æ•°ã®è¿½åŠ ï¼ˆ.envï¼‰**:
```bash
# llama-cppç”¨ã®Flash Attentionè¨­å®š
OLLAMA_FLASH_ATTENTION=true  # æ³¨: ã“ã‚Œã¯Ollamaã®ç’°å¢ƒå¤‰æ•°ã§ã™

# KV Cacheé‡å­åŒ–ã‚¿ã‚¤ãƒ—
LLAMA_F16_KV=true
```

**åŠ¹æœ**:
- æ¨è«–é€Ÿåº¦ã®å¤§å¹…å‘ä¸Šï¼ˆFlash Attentionã«ã‚ˆã‚Šï¼‰
- VRAMä½¿ç”¨é‡ã®å‰Šæ¸›ï¼ˆKV Cacheé‡å­åŒ–ã«ã‚ˆã‚Šï¼‰
- ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã§é¡•è‘—

### å„ªå…ˆåº¦ä¸­ï¼ˆèª¿æŸ»ã¨å®Ÿè£…å¿…è¦ï¼‰

#### 4. ä¸¦åˆ—å‡¦ç†ã®å°å…¥

ç¾åœ¨ã®Hoshikageã¯ `asyncio.Semaphore(1)` ã«ã‚ˆã‚ŠåŒæ™‚å®Ÿè¡Œã‚’1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«åˆ¶é™ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚’ç·©å’Œã—ã¦ä¸¦åˆ—å‡¦ç†ã‚’å¯èƒ½ã«ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³A: ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰+æ’ä»–åˆ¶å¾¡ã®å¼·åŒ–**
```python
from concurrent.futures import ThreadPoolExecutor

class ModelManager:
    def __init__(self):
        self.llm: Optional[Llama] = None
        self.lock = asyncio.Lock()
        self.max_parallel = int(os.getenv("MAX_PARALLEL", "1"))  # è¨­å®šå¯èƒ½

    async def generate(self, prompt, options):
        """æ’ä»–åˆ¶å¾¡ä»˜ãã®ä¸¦åˆ—ç”Ÿæˆ"""
        async with self.lock:
            # Llamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹è‡ªä½“ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã§ã¯ãªã„ãŸã‚
            # ã“ã“ã§ã¯éåŒæœŸå®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ThreadPoolExecutorã‚’ä½¿ç”¨
                self._sync_generate,
                prompt,
                options
            )
            return result

    def _sync_generate(self, prompt, options):
        """åŒæœŸçš„ãªç”Ÿæˆé–¢æ•°"""
        return self.llm(prompt, **options)
```

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³B: ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆOllamaæ–¹å¼ï¼‰**
- Hoshikageã‚’Goã§æ›¸ãç›´ã™ã‹ã€llama.cppã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦å®Ÿè¡Œ
- HTTPã‚µãƒ¼ãƒãƒ¼ã‚’ç«‹ã¡ä¸Šã’ã€è¤‡æ•°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨±å®¹
- å®Ÿè£…ã‚³ã‚¹ãƒˆã¯é«˜ã„ãŒã€æœ€ã‚‚å …ç‰¢ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

#### 5. å‹•çš„ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ

llama-cpp-python APIã«ã¯é™å®šçš„ãªãƒ¡ãƒ¢ãƒªåˆ¶å¾¡ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®æ–¹æ³•ã§æ”¹å–„å¯èƒ½ã§ã™ã€‚

**A. llama-cppã®ãƒã‚¤ãƒ†ã‚£ãƒ–APIã‚’ä½¿ç”¨**
```python
from llama_cpp import Llama, llama_cpp

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å‰ã«ç©ºããƒ¡ãƒ¢ãƒªã‚’ç¢ºèª
def check_gpu_memory_availability():
    import subprocess
    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.free,memory.total', '--format=csv,noheader'],
                          capture_output=True, text=True)
    # è§£æã—ã¦åˆ¤æ–­...
    return free_memory

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦n_gpu_layersã‚’å‹•çš„ã«èª¿æ•´
model_size_bytes = os.path.getsize(model_path)
available_vram = check_gpu_memory_availability()

# ç°¡æ˜“ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
if available_vram < model_size_bytes * 0.8:
    n_gpu_layers = -1  # å…¨ã¦CPUï¼ˆå®‰å…¨ç­–ï¼‰
else:
    n_gpu_layers = int(available_vram / (model_size_bytes / 32))  # ç´„1/3ã‚’GPUã«

self.llm = Llama(
    model_path=ram_model_path,
    n_gpu_layers=n_gpu_layers,  # å‹•çš„è¨­å®š
    # ... other params
)
```

**B. llama-cppã®CGOæ‹¡å¼µã‚’ä½¿ç”¨**
- llama-cppã‚’ç›´æ¥ãƒ“ãƒ«ãƒ‰ã—ã€Hoshikageã‹ã‚‰CGOçµŒç”±ã§å‘¼ã³å‡ºã—
- Ollamaã¨åŒã˜ãƒ¬ãƒ™ãƒ«ã®åˆ¶å¾¡ãŒå¯èƒ½ã«ãªã‚‹

---

## æŠ€è¡“çš„å·®åˆ†ã®æ·±æ˜ã‚Š

### llama-cpp-python ã¨ llama-cppï¼ˆCGOï¼‰ã®é•ã„

| é …ç›® | llama-cpp-python | llama-cppï¼ˆOllamaã§ä½¿ç”¨ï¼‰ |
|------|-------------------|----------------------------|
| **API** | é«˜ãƒ¬ãƒ™ãƒ«Pythonãƒ©ãƒƒãƒ‘ãƒ¼ | ä½ãƒ¬ãƒ™ãƒ«C/C++ API |
| **åˆ¶å¾¡ç´°åº¦** | é™å®šçš„ | é«˜åº¦ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼å˜ä½ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¶å¾¡ï¼‰ |
| **Flash Attention** | ãƒãƒ¼ã‚¸ãƒ§ãƒ³/ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¾å­˜ | å‹•çš„åˆ‡æ›¿å¯èƒ½ |
| **KV Cacheé‡å­åŒ–** | åŸºæœ¬çš„ãªãƒ•ãƒ©ã‚°ã®ã¿ | è¤‡æ•°ã®é‡å­åŒ–ã‚¿ã‚¤ãƒ—é¸æŠå¯èƒ½ |
| **ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ** | å˜ç´”ãªn_gpu_layersè¨­å®š | ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–ãªæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| **ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢** | Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿å†… | ç‹¬è‡ªãƒ—ãƒ­ã‚»ã‚¹ |

### ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¤‡é›‘ã•

Ollamaã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã¯ä»¥ä¸‹ã®çŠ¶æ…‹é·ç§»ã‚’ç®¡ç†ã—ã¾ã™ï¼š

1. **æ–°è¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—é ˜**
   - æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ç¢ºèª
   - å¿…è¦ãªã‚‰å†ãƒ­ãƒ¼ãƒ‰åˆ¤å®šï¼ˆ`needsReload`ï¼‰

2. **ãƒ¡ãƒ¢ãƒªé©åˆæ€§ç¢ºèª**
   - `LoadOperationFit` ã§ãƒ¡ãƒ¢ãƒªè¦ä»¶ã®ã¿ã‚’è¨ˆç®—
   - æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¨ã®å…±å­˜ã‚’ç¢ºèª

3. **ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦**
   - `LoadOperationAlloc` ã§å®Ÿéš›ã®ãƒ¡ãƒ¢ãƒªç¢ºä¿
   - ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’GPU/CPUã«é…ç½®

4. **ã‚³ãƒŸãƒƒãƒˆ**
   - `LoadOperationCommit` ã§ã‚¦ã‚§ã‚¤ãƒˆãƒ­ãƒ¼ãƒ‰
   - ä½¿ç”¨é–‹å§‹

5. **ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†**
   - å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã®å¢—æ¸›
   - ã‚¢ã‚¤ãƒ‰ãƒ«æ¤œå‡ºã¨ã‚¿ã‚¤ãƒãƒ¼è¨­å®š

6. **æœŸé™åˆ‡ã‚Œ**
   - ã‚»ãƒƒã‚·ãƒ§ãƒ³æœŸé–“çµ‚äº†
   - ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨VRAMå›å¾©å¾…æ©Ÿ

---

## Hoshikageã®ç‹¬è‡ªæ©Ÿèƒ½ã®è©•ä¾¡

Hoshikageã¯Ollamaã«ã¯ãªã„ç‹¬è‡ªã®æœ€é©åŒ–æ©Ÿèƒ½ã‚‚æŒã£ã¦ã„ã¾ã™ã€‚

### è‰¯ã„ç‚¹

1. **æ–‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è¦ç´„**
   - æ–‡ãƒ¬ãƒ™ãƒ«ã§ã®é¡ä¼¼åº¦è¨ˆç®—
   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ä»£è¡¨çš„ãªæ–‡ã®ã¿ã‚’æŠ½å‡º
   - ChromaDBã«ã‚ˆã‚‹é‡è¤‡æ’é™¤
   - é•·ã„ä¼šè©±å±¥æ­´ã®åŠ¹ç‡çš„ãªåœ§ç¸®

2. **RAMãƒ‡ã‚£ã‚¹ã‚¯ã«ã‚ˆã‚‹é«˜é€ŸåŒ–**
   - ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
   - ãƒ‡ã‚£ã‚¹ã‚¯I/Oã®å‰Šæ¸›

3. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ**
   - Server-Sent Eventsï¼ˆSSEï¼‰ã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã®å‘ä¸Š

---

## æ¨å¥¨ã•ã‚Œã‚‹å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### ãƒ•ã‚§ãƒ¼ã‚º1ï¼šå³æ™‚æ”¹å–„ï¼ˆ1-2é€±é–“ï¼‰

1. âœ… VRAMå›å¾©å¾…æ©Ÿæ©Ÿæ§‹ã®å®Ÿè£…
2. âœ… å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã®å¼·åŒ–
3. âœ… Flash Attentionã¨KV Cacheé‡å­åŒ–ã®æœ‰åŠ¹åŒ–
4. âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ›´æ–°

### ãƒ•ã‚§ãƒ¼ã‚º2ï¼šä¸­æœŸçš„æ”¹å–„ï¼ˆ1-2ãƒ¶æœˆï¼‰

1. ğŸ”„ å‹•çš„ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å°å…¥ï¼ˆnvidia-smié€£æºï¼‰
2. ğŸ”„ ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…ï¼ˆThreadPoolExecutorãƒ¢ãƒ‡ãƒ«ï¼‰
3. ğŸ”„ GPUãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›£è¦–ã¨è¨˜éŒ²
4. ğŸ”„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### ãƒ•ã‚§ãƒ¼ã‚º3ï¼šé•·æœŸçš„æ¤œè¨ï¼ˆ3ãƒ¶æœˆä»¥ä¸Šï¼‰

1. ğŸ”­ llama-cppã‚’ç›´æ¥ä½¿ç”¨ã™ã‚‹ã‹æ¤œè¨
   - CGOãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®ä½œæˆ
   - ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã¸ã®ç§»è¡Œ

2. ğŸ”­ ãƒãƒ«ãƒGPUå¯¾å¿œã®å¼·åŒ–
   - GPUã”ã¨ã®å‹•çš„ãƒ¬ã‚¤ãƒ¤ãƒ¼å‰²ã‚Šå½“ã¦
   - GPUé–“é€šä¿¡ã®æœ€é©åŒ–

3. ğŸ”­ ç‹¬è‡ªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®å®Ÿè£…
   - Ollamaã®æ–¹å¼ã‚’å‚è€ƒã«ã—ãŸç‹¬è‡ªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

4. ğŸ”­ KV Cacheã®é«˜åº¦ãªæœ€é©åŒ–
   - ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã®æœ€é©åŒ–

---

## çµè«–

### Hoshikageã®å¼·ã¿

1. **ã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**
   - å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmain.pyï¼‰ã§å…¨ã¦ã®ãƒ­ã‚¸ãƒƒã‚¯
   - Pythonã«ã‚ˆã‚‹è¿…é€Ÿãªé–‹ç™º

2. **é«˜åº¦ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†**
   - æ–‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªè¦ç´„
   - ChromaDBã«ã‚ˆã‚‹é‡è¤‡æ’é™¤

3. **æŸ”è»Ÿãªè¨­å®š**
   - ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹å‹•çš„è¨­å®š
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ç’°å¢ƒã«åˆã‚ã›ãŸèª¿æ•´ãŒå®¹æ˜“

### Hoshikageã®å¼±ç‚¹

1. **LLMã‚¨ãƒ³ã‚¸ãƒ³ã®åˆ¶é™**
   - llama-cpp-pythonã¯é«˜åº¦ãªæ©Ÿèƒ½ã«ã‚¢ã‚¯ã‚»ã‚¹å›°é›£
   - ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåˆ¶å¾¡ãŒé™å®šçš„

2. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®åˆ¶é™**
   - å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã€å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã¿
   - è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åŒæ™‚åˆ©ç”¨ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ãªã„

3. **VRAMç®¡ç†ã®ä¸å‚™**
   - GPUãƒ¡ãƒ¢ãƒªè§£æ”¾ã®å¾…æ©ŸãŒãªã„
   - å¤§ããªãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã§å¤±æ•—ã®å¯èƒ½æ€§

### Ollamaã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‹ã‚‰å­¦ã¹ã¹ãç‚¹

1. **å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**
   - ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢ã«ã‚ˆã‚Šã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®å¾©æ—§å®¹æ˜“
   - HTTPã‚¨ãƒ©ãƒ¼ã¨ãƒ—ãƒ­ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼ã®æ˜ç¤ºçš„åŒºåˆ¥

2. **ç¶¿å¯†ãªç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°**
   - VRAMä½¿ç”¨é‡ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
   - è©³ç´°ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹å ±å‘Š

3. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­å¿ƒã®è¨­å®š**
   - Modelfileã«ã‚ˆã‚‹å®£è¨€çš„ãªè¨­å®š
   - ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹å¾®ç´°ãªåˆ¶å¾¡

4. **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æœ€é©åŒ–**
   - å„OSå‘ã‘ã®ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒã‚¤ãƒŠãƒª
   - Metal, CUDA, ROCm, Vulkanã®æœ€é©ã‚µãƒãƒ¼ãƒˆ

---

## å‚è€ƒè³‡æ–™

### Ollamaã®ä¸»è¦ãªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«

1. **server/sched.go** (ç´„31,000è¡Œ)
   - ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ä¸­æ ¸å®Ÿè£…
   - VRAMå›å¾©å¾…æ©Ÿã€ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰æˆ¦ç•¥

2. **llm/server.go** (ç´„56,000è¡Œï¼‰
   - LLMã‚µãƒ¼ãƒãƒ¼ã®å®Ÿè£…
   - ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€Flash Attentionã€KV Cache

3. **kvcache/cache.go**
   - KV Cacheã®ç‹¬è‡ªå®Ÿè£…
   - ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚µãƒãƒ¼ãƒˆ

4. **runner/llamarunner** / **runner/ollamarunner**
   - llama.cppã®ãƒ©ãƒƒãƒ‘ãƒ¼å®Ÿè£…
   - ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã®èµ·å‹•ã¨ç®¡ç†

### é–¢é€£æŠ€è¡“

- **llama.cpp**: https://github.com/ggerganov/llama.cpp
- **Flash Attention**: https://arxiv.org/abs/2307.08691
- **KV Cache Quantization**: https://github.com/ggerganov/llama.cpp/pull/3953
- **GGML Format**: OllamaãŒä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

---

**ä½œæˆè€…**: Code Analysis Agent
**å¯¾è±¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³**:
- Hoshikage: 0.1.0
- Ollama: mainãƒ–ãƒ©ãƒ³ãƒï¼ˆ2026-01-16æ™‚ç‚¹ï¼‰

---

*æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯ã€Ollamaã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’åˆ†æã—ãŸçµæœã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚*
