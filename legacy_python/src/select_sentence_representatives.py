import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import math
import re
import logging

logger = logging.getLogger(__name__)

def split_and_clean_sentences(text: str) -> list[str]:
    """
    æ˜Ÿå½±è¦ç´„ç”¨ï¼šã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ä¿è­· + è‹±æ–‡ã¨æ—¥æœ¬èªã®åˆ†å‰² + ãƒã‚¤ã‚ºé™¤å»å‡¦ç†

    Parameters:
    - text (str): ä¼šè©±å±¥æ­´ãªã©ã®ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
    - List[str]: ã‚¯ãƒªãƒ¼ãƒ³ãªæ–‡å˜ä½ã®ãƒªã‚¹ãƒˆï¼ˆæœ«å°¾å¥ç‚¹ãªã—ãƒ»è‹±å­—ãƒã‚¤ã‚ºé™¤å»æ¸ˆï¼‰
    """

    # === â‘  ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®æŠ½å‡º ===
    code_blocks = {}
    def extract_code_block(match):
        key = f"<CODE_BLOCK_{len(code_blocks)}_CODE_BLOCK>"
        code_blocks[key] = match.group(0)
        return key

    text = re.sub(r"```(?:python)?\n.*?```", extract_code_block, text, flags=re.DOTALL)

    # === â‘¡ æ”¹è¡Œã§ä¸€æ¬¡åˆ†å‰² ===
    lines = text.splitlines()

    cleaned_sentences = []

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã¯ãã®ã¾ã¾
        if re.fullmatch(r"<CODE_BLOCK_\d+_CODE_BLOCK>", line):
            cleaned_sentences.append(code_blocks[line])
            continue

        # === â‘¢ è‹±æ–‡åˆ¤å®šï¼š.?!ã§çµ‚ã‚ã‚Š & æ—¥æœ¬èªãªã— ===
        if re.search(r'[.?!]$', line) and not re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]', line):
            # è‹±æ–‡ â†’ .?! + ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²ï¼ˆè‡ªç„¶ãªåˆ†å‰²ï¼‰
            english_fragments = re.split(r'(?<=[.?!])\s+', line)
            for frag in english_fragments:
                frag = frag.strip()
                if frag and not re.fullmatch(r'[^\w]+', frag):
                    cleaned_sentences.append(frag)
            continue

        # === â‘£ æ—¥æœ¬èªå‡¦ç†ï¼šã€‚ã¾ãŸã¯æ”¹è¡Œã§åˆ†å‰²ï¼ˆå¥ç‚¹é™¤å»ï¼‰===
        jp_fragments = re.split(r'[ã€‚]', line)
        for frag in jp_fragments:
            frag = frag.strip()
            # ãƒã‚¤ã‚ºè‹±å­—è¡Œã‚‚ã“ã“ã§é™¤å»
            if frag and not re.fullmatch(r'[^\wã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥a-zA-Z0-9]+', frag):
                # è‹±å­—ã®ã¿ï¼ˆè¨˜å·é™¤å¤–ï¼‰ã®å ´åˆã¯å‰Šé™¤
                if re.fullmatch(r'[a-zA-Z0-9_()[\]{}\-+=:;\"\'*.,<>/@\\\s]+', frag):
                    continue  # â† ãƒã‚¤ã‚ºãªã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼

                # ğŸ”¥ æ„å‘³ä¸æ˜é˜²æ­¢ï¼š15æ–‡å­—ä»¥ä¸‹ã®æ–‡ã¯é™¤å¤–
                if len(frag) <= 15:
                    continue
                cleaned_sentences.append(frag)

    return cleaned_sentences

def is_english_line(line: str) -> bool:
    """
    ãŸã­ã¡ã‚ƒã‚“å¼ï¼šè‹±æ–‡åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæœ«å°¾ãŒ .!? ã®ã¿ / æ—¥æœ¬èªæ–‡å­—ã‚’å«ã¾ãªã„ï¼‰
    """
    line = line.strip()
    return (
        re.search(r'[.?!]$', line) is not None and   # â† æ–‡æœ«ãŒ .!? ã®ã„ãšã‚Œã‹
        not re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]', line)     # â† æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„
    )

def format_clustered_representatives(representatives: list[tuple[int, int, str]]) -> str:
    """
    ã‚¯ãƒ©ã‚¹ã‚¿ä»£è¡¨æ–‡ãƒªã‚¹ãƒˆã‚’ Markdown å½¢å¼ã«æ•´å½¢ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿é–“ã« --- ã‚’æŒ¿å…¥ï¼‰

    Parameters:
    - representatives: (cluster_id, original_index, sentence) ã®ãƒªã‚¹ãƒˆï¼ˆã‚¯ãƒ©ã‚¹ã‚¿é †ã§ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰

    Returns:
    - str: Markdownå½¢å¼ã®è¦ç´„æ–‡å­—åˆ—
    """
    markdown = []
    current_cluster = None

    for cluster_id, _, sentence in representatives:
        if cluster_id != current_cluster:
            if current_cluster is not None:
                markdown.append('---')
            current_cluster = cluster_id
        markdown.append(f"- {sentence.strip()}")

    return '\n'.join(markdown)

def select_sentence_representatives(
    sentences: list[str],
    embedder,
    cluster_divisor: int = 100,
    min_clusters: int = 1,
    max_clusters: int = 20
) -> list[str]:
    """
    æ„å‘³ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ä»£è¡¨æ–‡æŠ½å‡ºï¼ˆç­‰é–“éš” + æ„å‘³é †ã‚½ãƒ¼ãƒˆï¼‰

    Parameters:
    - sentences (List[str]): åˆ†å‰²ãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ã®æ–‡ãƒªã‚¹ãƒˆ
    - embedder (Callable): __call__ ã§æ–‡ãƒªã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§ãã‚‹é–¢æ•°ï¼ˆruriã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼‰
    - cluster_divisor (int): ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®é™¤æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
    - min_clusters (int): æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
    - max_clusters (int): æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰

    Returns:
    - List[str]: ã‚¯ãƒ©ã‚¹ã‚¿å˜ä½ã§æ„å‘³é †ã«ä¸¦ã¹ã‚‰ã‚ŒãŸä»£è¡¨æ–‡ãƒªã‚¹ãƒˆ
    """
    try:
        if not sentences:
            return []

        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        vecs = np.array(embedder(sentences))

        # ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ±ºå®š
        k = max(min_clusters, min(max_clusters, len(sentences) // cluster_divisor))
        logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {k}ï¼ˆæ–‡æ•°: {len(sentences)}ï¼‰")

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        kmeans = KMeans(n_clusters=k, n_init="auto", random_state=42)
        cluster_ids = kmeans.fit_predict(vecs)

        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«ä»£è¡¨æ–‡ã‚’æŠ½å‡ºï¼ˆæ„å‘³é †å‡ºåŠ›ã®ãŸã‚ã‚¯ãƒ©ã‚¹ã‚¿é †ã«ã¾ã¨ã‚ã‚‹ï¼‰
        representatives = []
        for cluster_id in range(k):
            indices = [i for i, cid in enumerate(cluster_ids) if cid == cluster_id]
            if not indices:
                continue

            cluster_vecs = vecs[indices]
            centroid = kmeans.cluster_centers_[cluster_id].reshape(1, -1)
            sims = cosine_similarity(cluster_vecs, centroid).flatten()
            sorted_pairs = sorted(zip(sims, indices), reverse=True)

            n_extract = min(math.ceil(len(indices) / 10), 5)
            logger.debug(f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®æ–‡æ•°: {len(indices)}ã€ä»£è¡¨æ–‡æ•°: {n_extract}")
            step = max(1, len(sorted_pairs) // n_extract)

            selected = [sorted_pairs[i * step][1] for i in range(n_extract)]
            # (ã‚¯ãƒ©ã‚¹ã‚¿ID, å…ƒæ–‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, æ–‡) ã‚’ä¿å­˜
            representatives.extend((cluster_id, i, sentences[i]) for i in selected)

        # ã‚¯ãƒ©ã‚¹ã‚¿é † â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é † ã§ä¸¦ã¹æ›¿ãˆ
        representatives.sort(key=lambda x: (x[0], x[1]))

        return format_clustered_representatives(representatives)
    
    except ValueError as e:
        logger.error(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¸­ã«å€¤ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¤±æ•—æ™‚ã¯ã€ç­‰é–“éš”ã§ä»£è¡¨æ–‡ã‚’æŠ½å‡º
        logger.info("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«å¤±æ•—ã—ãŸãŸã‚ã€ç­‰é–“éš”ã§ä»£è¡¨æ–‡ã‚’æŠ½å‡ºã—ã¾ã™")
        step = max(1, len(sentences) // 10)
        selected_indices = list(range(0, len(sentences), step))[:10]
        representatives = [(0, i, sentences[i]) for i in selected_indices]
        return format_clustered_representatives(representatives)
    except Exception as e:
        logger.error(f"ä»£è¡¨æ–‡æŠ½å‡ºä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise
