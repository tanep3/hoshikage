'''
hoshikage.py ã¯ã€æ˜Ÿå½±ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã‚’è¡Œã†ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¨ä½ç½®ã¥ã‘ã¾ã™ã€‚
ä½¿ã„æ–¹ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
python hoshikage.py [command] args...
commandã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ãŒå¿…è¦ã€‚
ï¼‘ï¼add: ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã™ã‚‹ã€‚
ä½¿ã„æ–¹ï¼š python hoshikage.py add [ç™»éŒ²ç”¨ã®ãƒ¢ãƒ‡ãƒ«å] [ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹]
å‡¦ç†ï¼šmodel_map.jsonã€tags_cache.json ã«ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã™ã‚‹ã€‚ã‚‚ã—ã€ãƒ¢ãƒ‡ãƒ«åãŒç™»éŒ²æ¸ˆã¿ã ã£ãŸã‚‰ã€ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹ã€‚
ï¼’ï¼remove: ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ã™ã‚‹ã€‚
ä½¿ã„æ–¹ï¼špython hoshikage.py remove [ãƒ¢ãƒ‡ãƒ«å]
ï¼“ï¼list: ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤ºã™ã‚‹ã€‚å¿…è¦ãªæƒ…å ±ã¯ã€ãƒ¢ãƒ‡ãƒ«åã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã€‚
'''

import os
import json
import hashlib
import datetime
import sys
from llama_cpp import Llama
from dotenv import load_dotenv
import pathlib

# .env èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ï¼‰
project_root = pathlib.Path(__file__).parent.parent
load_dotenv(dotenv_path=project_root / ".env")

def get_env_path(key, default_rel_path):
    val = os.getenv(key)
    if val:
        if val.startswith("./"):
            return str(project_root / val[2:])
        return val
    return str(project_root / default_rel_path)

MODEL_MAP_FILE = get_env_path("MODEL_MAP_FILE", "data/model_map.json")
TAG_CACHE_FILE = get_env_path("TAG_CACHE_FILE", "data/tags_cache.json")
TAG_OLLAMA_FILE = get_env_path("TAG_OLLAMA_FILE", "data/tags_ollama.json")


def load_json(filepath):
    if not os.path.exists(filepath):
        return {}
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)


def save_json(filepath, data):
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def format_openai_model(name):
    return {
      "id": name,
      "object": "model",
      "created": 1686935002,
      "owned_by": "tane"
    }

def get_file_metadata(full_path):
    stat = os.stat(full_path)
    size = stat.st_size
    modified_at = datetime.datetime.fromtimestamp(stat.st_mtime).isoformat()
    with open(full_path, "rb") as f:
        digest = hashlib.sha256(f.read()).hexdigest()
    return size, modified_at, digest

def format_ollama_model(name, full_path):
    # llm = Llama(model_path=full_path, vocab_only=True, verbose=False)
    # meta = llm.metadata
    size, modified_at, digest = get_file_metadata(full_path)

    return {
        "name": name,
        "model": name + ":latest",
        "modified_at": modified_at,
        "size": size,
        "digest": digest,
        "details": {
            # "parent_model": "",
            "format": "gguf",
            "family": "llama",
            "families": "null",
            "parameter_size": "12B",
            "quantization_level": "Q4_0"
        }
    }


def add_model(model_path, model_alias):
    if not os.path.exists(model_path):
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}")
        return

    model_name = os.path.basename(model_path)
    model_dir = os.path.dirname(model_path)

    model_map = load_json(MODEL_MAP_FILE)
    tags_cache = load_json(TAG_CACHE_FILE).get("data", [])
    tags_ollama = load_json(TAG_OLLAMA_FILE).get("models", [])

    if model_alias in model_map:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«å '{model_alias}' ã¯ã™ã§ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚")
        return

    model_map[model_alias] = {"path": model_dir, "model": model_name}
    formatted = format_openai_model(model_alias)
    formatted_ollama = format_ollama_model(model_alias, model_path)
    tags_cache.append(formatted)
    tags_json = {
        "object": "list",
        "data": tags_cache,
    }
    tags_ollama.append(formatted_ollama)
    tags_ollama_json = {
        "models": tags_ollama,
    }
    save_json(MODEL_MAP_FILE, model_map)
    save_json(TAG_CACHE_FILE, tags_json)
    save_json(TAG_OLLAMA_FILE, tags_ollama_json)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ« '{model_alias}' ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")


def remove_model(model_alias):
    model_map = load_json(MODEL_MAP_FILE)
    tags_cache = load_json(TAG_CACHE_FILE).get("data", [])
    tags_ollama = load_json(TAG_OLLAMA_FILE).get("models", [])

    if model_alias not in model_map:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model_alias}' ã¯ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    del model_map[model_alias]
    tags_cache = [m for m in tags_cache if m["id"] != model_alias]
    tags_json = {
        "object": "list",
        "data": tags_cache,
    }
    tags_ollama = [m for m in tags_ollama if m["name"] != model_alias]
    tags_ollama_json = {
        "models": tags_ollama,
    }

    save_json(MODEL_MAP_FILE, model_map)
    save_json(TAG_CACHE_FILE, tags_json)
    save_json(TAG_OLLAMA_FILE, tags_ollama_json)
    print(f"ğŸ—‘ï¸ ãƒ¢ãƒ‡ãƒ« '{model_alias}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")


def list_models():
    model_map = load_json(MODEL_MAP_FILE)
    print(f"ğŸ“¦ ç™»éŒ²æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ï¼ˆ{len(model_map)}ä»¶ï¼‰:")
    for alias, conf in model_map.items():
        model_path = os.path.join(conf["path"], conf["model"])
        size = os.path.getsize(model_path) if os.path.exists(model_path) else 0
        print(f" - {alias}: {model_path} ({size / 1024 / 1024:.2f} MB)")

def usage():
    print("ä½¿ã„æ–¹: python hoshikage.py [add|remove|list] ...")
    print("add [ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹] [alias]")
    print("remove [alias]")
    print("list")
    return

def main():
    if len(sys.argv) < 2:
        usage()
        return

    command = sys.argv[1]
    if command == "add" and len(sys.argv) == 4:
        add_model(sys.argv[2], sys.argv[3])
    elif command == "remove" and len(sys.argv) == 3:
        remove_model(sys.argv[2])
    elif command == "list":
        list_models()
    else:
        print("âŒ ã‚³ãƒãƒ³ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        usage()

main()