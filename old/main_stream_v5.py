import os
import asyncio
import time
import logging
from typing import List, Optional, Literal
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from starlette.responses import Response
from dotenv import load_dotenv
from llama_cpp import Llama
# from llama_cpp.llama_speculative import LlamaPromptLookupDecoding
import mount as mt
from uuid import uuid4
from models.schema import ChatCompletionRequest, ChatSessionManager
import json
import gc

VERSION = "0.1.0"

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env èª­ã¿è¾¼ã¿
load_dotenv()
RAMDISK_PATH = os.getenv("RAMDISK_PATH", "/mnt/temp/hoshikage")
IDLE_TIMEOUT = int(os.getenv("IDLE_TIMEOUT_SECONDS", "300"))
GREAT_TIMEOUT = int(os.getenv("GREAT_TIMEOUT", "60")) * 60
MODEL_MAP_FILE = os.getenv("MODEL_MAP_FILE", "./models/model_map.json")
TAG_CACHE_FILE = os.getenv("TAG_CACHE_FILE", "./models/tags_cache.json")

llm = None
llm_lock = asyncio.Lock()
last_access_time = time.time()
chat_session_manager = ChatSessionManager()
current_model = ""

async def initialize_model(model_alias):
    global llm, current_model
    if model_alias == current_model:
        if llm is not None:
            return
    current_model = model_alias
    ram_model_path = mt.get_model(current_model)
    if llm is not None:
        llm.close()
    llm = Llama(
        model_path=ram_model_path, 
        # n_ctx=2096,
        # n_ctx=122880,
        # n_ctx=61440,
        n_ctx=10240,
        # n_ctx=4096,     # æ–‡è„ˆé•·ï¼šé•·ã‚ã§ã‚‚OKï¼ˆ4096ãŒæ¨å¥¨æœ€å¤§ï¼‰
        n_threads=12,    # Ryzen 7900ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã«å¿œã˜ã¦ï¼ˆä¸Šé™ã¯è‡ªå‹•ã§ã‚‚è‰¯ã„ï¼‰
        n_gpu_layers=-1, # -1ã¯GPUã‚’Maxã¾ã§ä½¿ã†
        n_batch=512,         # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå¤§ãã„ã¨é«˜é€Ÿãƒ»ãŸã ã—VRAMã«æ³¨æ„ï¼‰
        # low_vram=True,
        # use_mlock=False,
        # embedding=True, #ã“ã‚Œçµ¶å¯¾ã«ã ã‚ã€‚è½ã¡ã‚‹ã€‚
        # draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10), #ã“ã‚Œã‚‚ã ã‚ã€‚ã‚¨ãƒ©ãƒ¼å‡ºã‚‹ã€‚å›ç­”ãŒé€”åˆ‡ã‚Œã‚‹ã€‚
        verbose=False    # ğŸ‘ˆ å‡ºåŠ›ã‚’æŠ‘åˆ¶
    )
    gc.collect()

async def check_idle_timeout():
    global llm, last_access_time
    if time.time() - last_access_time > IDLE_TIMEOUT:
        async with llm_lock:
            if llm:
                logger.info("â³ éã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ™‚é–“è¶…éã®ãŸã‚ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
                llm.close()  # ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
                llm = None  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚¿ãŒè§£æ”¾ã—ã‚„ã™ãã™ã‚‹
                gc.collect()
    if time.time() - last_access_time > GREAT_TIMEOUT:
        mt.unmount_ramdisk(RAMDISK_PATH)

@app.get("/v1/status")
async def status():
    return {"status": "ok"}

# @app.get("/api/tags")
@app.get("/v1/models")
async def get_model_tags():
    try:
        if not os.path.exists(TAG_CACHE_FILE):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {TAG_CACHE_FILE}")
        with open(TAG_CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"/api/tags ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "code": "tags_fetch_failed",
                    "message": str(e),
                    "type": "internal_server_error"
                }
            }
        )

@app.get("/v1/api/version")
async def get_version():
    return {"version": VERSION}

# @app.post("/api/chat")
@app.post("/v1/chat/completions")
async def create_completion(completion_data: ChatCompletionRequest):
    global last_access_time, chat_session_manager
    async with llm_lock:
        model_alias = completion_data.model
        await initialize_model(model_alias)
        last_access_time = time.time()

        messages = completion_data.messages
        session_id = messages[0].session_id if hasattr(messages[0], "session_id") else "default_session"

        # ğŸ§  æœ€æ–°ã®userç™ºè¨€ã ã‘ã‚’ã€RAGå±¥æ­´ã¨ã—ã¦ä¿æŒã—ã¦ãŠã
        latest_user_msg = None
        for i in range(len(messages)-1, -1, -1):
            if messages[i].role == "user":
                latest_user_msg = messages[i]
        if latest_user_msg:
            chat_session_manager.add_message(session_id, "user", latest_user_msg.content)

        prompt = ""
        for message in messages:
            prompt += f"{message.role}: {message.content}\n"
        prompt += "assistant: "

        # âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿å®šç¾©
        def stream_generator():
            try:
                partial_text = ""
                for chunk in llm(
                    prompt,
                    max_tokens=2096,
                    stop=["<|eot|>"],
                    # stop=["<|eot|>", "user:", "User:", "Assistant:", "assistant:"],
                    stream=True
                ):
                    delta = chunk.get("choices", [{}])[0].get("text", "")
                    # delta = chunk["choices"][0]["text"]
                    if not delta:
                        continue
                    partial_text += delta
                    # OpenAIäº’æ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    payload = {
                        "id": f"chatcmpl-{uuid4().hex}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model_alias,
                        "choices": [{
                            "delta": {"content": delta},
                            "index": 0,
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(payload)}\n\n"
                # âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®çµ‚äº†é€šçŸ¥
                yield "data: [DONE]\n\n"
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                chat_session_manager.add_message(session_id, "assistant", partial_text)

            except Exception as e:
                logger.error(f"Streaming error: {e}")
                yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"

        # st =  StreamingResponse(stream_generator(), media_type="text/event-stream")
        # st =  stream_generator()
        # logger.info("#############")
        # return st
        # return Response(stream_generator(), media_type="text/event-stream")
        return StreamingResponse(stream_generator(), media_type="text/event-stream")

        # def wrapped_stream():
        #     try:
        #         for chunk in stream_generator():
        #             yield chunk
        #             gc.collect()
        #     except GeneratorExit:
        #         logger.warning("ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ‡æ–­ã—ã¾ã—ãŸ")
        #     except Exception as e:
        #         logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸­ã«ä¾‹å¤–: {e}")
        #         yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"

        # return StreamingResponse(wrapped_stream(), media_type="text/event-stream")

        # queue = asyncio.Queue()

        # async def producer():
        #     for chunk in llm(
        #         prompt,
        #         max_tokens=2096,
        #         stop=["<|eot|>"], 
        #         stream=True):
        #         await queue.put(chunk)
        #     await queue.put("[DONE]")

        # async def consumer():
        #     while True:
        #         item = await queue.get()
        #         if item == "[DONE]":
        #             yield "data: [DONE]\n\n"
        #             break
        #         delta = item["choices"][0]["text"]
        #         payload = {
        #             "id": f"chatcmpl-{uuid4().hex}",
        #             "object": "chat.completion.chunk",
        #             "created": int(time.time()),
        #             "model": model_alias,
        #             "choices": [{
        #                 "delta": {"content": delta},
        #                 "index": 0,
        #                 "finish_reason": None
        #             }]
        #         }
        #         yield f"data: {json.dumps(payload)}\n\n"

        # asyncio.create_task(producer())
        # return StreamingResponse(consumer(), media_type="text/event-stream")

async def background_cleanup():
    while True:
        await asyncio.sleep(30) #30ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
        await check_idle_timeout()

@app.on_event("startup")
async def startup_event():
    asyncio.create_task(background_cleanup())

