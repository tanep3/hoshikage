import os
import asyncio
import time
import logging
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from dotenv import load_dotenv
from llama_cpp import Llama
import mount as mt
from uuid import uuid4
from models.schema import ChatCompletionRequest, ChatSessionManager
import json
import gc
import re

VERSION = "0.1.0"

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env èª­ã¿è¾¼ã¿
load_dotenv()
RAMDISK_PATH = os.getenv("RAMDISK_PATH", "/mnt/temp/hoshikage")
IDLE_TIMEOUT = int(os.getenv("IDLE_TIMEOUT_SECONDS", "300"))
GREAT_TIMEOUT = int(os.getenv("GREAT_TIMEOUT", "60")) * 60
MODEL_MAP_FILE = os.getenv("MODEL_MAP_FILE", "./models/model_map.json")
TAG_CACHE_FILE = os.getenv("TAG_CACHE_FILE", "./models/tags_cache.json")

llm = None
llm_lock = asyncio.Lock()
concurrency_semaphore = asyncio.Semaphore(1) # åŒæ™‚å®Ÿè¡Œæ•°ã‚’1ã«åˆ¶é™
last_access_time = time.time()
chat_session_manager = ChatSessionManager()
current_model = ""

async def initialize_model(model_alias):
    global llm, current_model
    if model_alias == current_model:
        if llm is not None:
            return
    current_model = model_alias
    ram_model_path = mt.get_model(current_model)
    if llm is not None:
        llm.close()
    llm = Llama(
        model_path=ram_model_path, 
        # n_ctx=2096,
        # n_ctx=122880,
        # n_ctx=61440,
        n_ctx=20960,
        # n_ctx=4096,     # æ–‡è„ˆé•·ï¼šé•·ã‚ã§ã‚‚OKï¼ˆ4096ãŒæ¨å¥¨æœ€å¤§ï¼‰
        n_threads=12,    # Ryzen 7900ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã«å¿œã˜ã¦ï¼ˆä¸Šé™ã¯è‡ªå‹•ã§ã‚‚è‰¯ã„ï¼‰
        n_gpu_layers=-1, # -1ã¯GPUã‚’Maxã¾ã§ä½¿ã†
        n_batch=1024,         # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå¤§ãã„ã¨é«˜é€Ÿãƒ»ãŸã ã—VRAMã«æ³¨æ„ï¼‰
        verbose=False    # ğŸ‘ˆ å‡ºåŠ›ã‚’æŠ‘åˆ¶
    )
    gc.collect()

async def check_idle_timeout():
    global llm, last_access_time
    if time.time() - last_access_time > IDLE_TIMEOUT:
        async with llm_lock:
            if llm:
                logger.info("â³ éã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ™‚é–“è¶…éã®ãŸã‚ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
                llm.close()  # ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
                llm = None  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚¿ãŒè§£æ”¾ã—ã‚„ã™ãã™ã‚‹
                gc.collect()
    if time.time() - last_access_time > GREAT_TIMEOUT:
        mt.unmount_ramdisk(RAMDISK_PATH)

@app.get("/v1/status")
async def status():
    return {"status": "ok"}

@app.get("/v1/models")
async def get_model_tags():
    try:
        if not os.path.exists(TAG_CACHE_FILE):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {TAG_CACHE_FILE}")
        with open(TAG_CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"/api/tags ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "code": "tags_fetch_failed",
                    "message": str(e),
                    "type": "internal_server_error"
                }
            }
        )

@app.get("/v1/api/version")
async def get_version():
    return {"version": VERSION}

# âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿å®šç¾©
async def stream_generator(current_model, prompt, session_id):
    try:
        partial_text = ""
        for chunk in llm(
            prompt,
            max_tokens=2096,
            stop=["<|eot|>", "user:"],
            # stop=["<|eot|>", "user:", "User:", "Assistant:", "assistant:"],
            stream=True
        ):
            delta = chunk.get("choices", [{}])[0].get("text", "")
            if not delta:
                continue
            partial_text += delta
            # OpenAIäº’æ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            payload = {
                "id": f"chatcmpl-{uuid4().hex}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": current_model,
                "choices": [{
                    "delta": {"content": delta},
                    "index": 0,
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(payload)}\n\n"
        # âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®çµ‚äº†é€šçŸ¥
        yield "data: [DONE]\n\n"
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        chat_session_manager.add_message(session_id, "assistant", partial_text)

    except Exception as e:
        logger.error(f"Streaming error: {e}")
        yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"

# éã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¤ãƒ³ã‚°ç”¨
async def non_streaming_generator(current_model, prompt, session_id):
    output = llm(
        prompt, 
        max_tokens=1024, 
        stop=["<|eot|>", "user:"],
    )
    assistant_message = output["choices"][0]["text"]
    chat_session_manager.add_message(session_id, "assistant", assistant_message)

    usage = output.get("usage", {})
    response = {
        "id": f"chatcmpl-{uuid4().hex}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": current_model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": assistant_message
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0)
        }
    }
    return response

def message_compress(message: str, role: str) -> str:
    if "```python" in message or role == "system":
        return message  # ã‚³ãƒ¼ãƒ‰ã‚„ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã¯ãã®ã¾ã¾ä¿æŒ
    if len(message) <= 150:
        return message  # 150æ–‡å­—ä»¥ä¸‹ã¯ç„¡åœ§ç¸®
    compress_prompt = f"system: æ¬¡ã®å†…å®¹ã‚’150æ–‡å­—ä»¥å†…ã«è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚æ–‡ç« ã®ä¸»ãŸã‚‹æ„å›³ã‚’å¤±ã‚ãªã„ã‚ˆã†ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚èªèª¿ã‚„è¨€ã„å›ã—ãªã©ä¼šè©±è¡¨ç¾çš„ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ãŒæœ‰ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ç¶­æŒã™ã‚‹ã‚ˆã†å¿ƒãŒã‘ã¦ä¸‹ã•ã„ã€‚å›ç­”ã¯è¦ç´„ã®ã¿ã‚’å‡ºåŠ›ã—ã€æ–‡å­—æ•°ãªã©ã®æƒ…å ±ã¯ä»˜ä¸ã—ãªã„ã“ã¨ã€‚Markdownè¡¨è¨˜ã¯å¹³æ–‡ã«ç›´ã—ã¦ä¸‹ã•ã„ã€‚åˆ¶å¾¡ã‚³ãƒ¼ãƒ‰ã¯å‰Šé™¤ã—ã¦ã€æ–‡å­—æ•°å‰Šæ¸›ã«å‹™ã‚ã‚‹ã“ã¨ã€‚\nuser: {message}\nassistant: "
    output = llm(
        compress_prompt, 
        max_tokens=256, 
        stop=["<|eot|>", "user:"],
    )
    # æ–‡æœ«ã®ã‚´ãƒŸã‚’é™¤å»ã—ã¦ã‹ã‚‰è¿”ã™
    result = re.sub(r"(```+|[\[\(\{]*$)", "", output["choices"][0]["text"]).strip()
    return result

@app.post("/v1/chat/completions")
async def create_completion(completion_data: ChatCompletionRequest):
    global last_access_time, chat_session_manager
    async with concurrency_semaphore:
        model_alias = completion_data.model
        await initialize_model(model_alias)
        last_access_time = time.time()
        # print("#########################")
        # print(f"model_alias: {model_alias}")
        # print(f"stream: {completion_data.stream}")
        # print(f"messages: {completion_data}")

        messages = completion_data.messages
        session_id = messages[0].session_id if hasattr(messages[0], "session_id") else "default_session"

        # ğŸ§  æœ€æ–°ã®userç™ºè¨€ã ã‘ã‚’ã€RAGå±¥æ­´ã¨ã—ã¦ä¿æŒã—ã¦ãŠã
        latest_user_msg = None
        # for i in range(len(messages)-1, -1, -1):
        #     if messages[i].role == "user":
        #         latest_user_msg = messages[i]
        #         break
        for msg in messages[::-1]:
            if msg.role == "user":
                latest_user_msg = msg
                break
        if latest_user_msg:
            chat_session_manager.add_message(session_id, "user", latest_user_msg.content)

        prompt = ""
        system_prompt = ""
        user_prompt = ""
        for msg in messages[::-1]:
            if msg.role == "system":
                system_prompt += msg.content
                continue
            if user_prompt == "":
                if msg.role == "user":
                    user_prompt = msg.content
                    continue
            compressed_message = message_compress(msg.content, msg.role)
            if msg.content != compressed_message:
                print("#########################")
                print(f"role: {msg.role}")
                print(f"original: {msg.content}")
                print("---")
                print(f"compress: {compressed_message}")
            prompt = f"{msg.role}: {compressed_message}\n" + prompt
            if len(prompt) > 5120:
                break
        if prompt:
            prompt = (
                "## **(å‚è€ƒæƒ…å ±)ä¼šè©±ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ** \n" 
                + prompt + 
                "\n## **æœ¬é¡Œï¼ˆä»¥ä¸‹ã«ä¼šè©±ã‚’ç¶šã‘ã¦ä¸‹ã•ã„ã€‚ï¼‰** \n"
            )
        if user_prompt:
            prompt += f"user: {user_prompt}\n"
        if system_prompt:
            prompt = f"system: {system_prompt}\n" + prompt
        prompt += "assistant: "
        print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
        print(prompt)

        if completion_data.stream:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆ
            return StreamingResponse(stream_generator(current_model, prompt, session_id), media_type="text/event-stream")
        else:
            # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆ
            # pre_messages = chat_session_manager.get_messages(session_id)
            # print(f"pre_messages: {pre_messages}")
            # for i in range(len(pre_messages)-1, -1, -1):
            #     if pre_messages[i].get("role"):
            #         prompt = f"{pre_messages[i].get("role")}: {pre_messages[i].get("content")}\n" + prompt
            # print("@@@@@@@@@@@@@@@@@@@@@@@@@@")
            # print(prompt)
            response = await non_streaming_generator(current_model, prompt, session_id)
            print(response)
            return response
        
async def background_cleanup():
    while True:
        await asyncio.sleep(30) #30ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
        await check_idle_timeout()

@app.on_event("startup")
async def startup_event():
    asyncio.create_task(background_cleanup())

